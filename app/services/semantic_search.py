import torch
import numpy as np
from typing import List, Dict, Any, Tuple
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

import re

from app.config.settings import settings


class SemanticSearchService:
    def __init__(self):
        self.model = SentenceTransformer(settings.EMBEDDINGS_MODEL)
        self.model.max_seq_length = 512

    def create_tender_text(self, tender: Dict[str, Any]) -> str:
        """–°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —Ç–µ–Ω–¥–µ—Ä–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞."""
        parts = []

        # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–Ω–¥–µ—Ä–∞
        name = tender.get('name', '').strip()
        if name:
            parts.append(f"–¢–æ–≤–∞—Ä {name}")

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        for char in tender.get('characteristics', []):
            if char.get('required', False):
                char_name = char.get('name', '').strip()
                char_value = self._clean_value(char.get('value', ''))

                if char_name and char_value:
                    parts.append(f"{char_name} {char_value}")

        return ". ".join(parts)

    def create_product_text(self, product: Dict[str, Any]) -> str:
        """–°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —Ç–æ–≤–∞—Ä–∞ —Å —É—á–µ—Ç–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
        parts = []

        # –ù–∞–∑–≤–∞–Ω–∏–µ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        title = product.get('title', '').strip()
        category = product.get('category', '').strip()

        if title:
            if category:
                parts.append(f"{category}: {title}")
            else:
                parts.append(title)

        # –î–æ–±–∞–≤–ª—è–µ–º –±—Ä–µ–Ω–¥
        brand = product.get('brand', '').strip()
        if brand:
            parts.append(f"–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å {brand}")

        # –¢–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
        if product.get('attributes'):
            key_attrs = []
            for attr in product['attributes'][:5]:  # –ú–∞–∫—Å–∏–º—É–º 5 –∞—Ç—Ä–∏–±—É—Ç–æ–≤
                attr_name = attr.get('attr_name', '').strip()
                attr_value = self._clean_value(attr.get('attr_value', ''))

                if attr_name and attr_value and len(attr_value) < 50:
                    key_attrs.append(f"{attr_name} {attr_value}")

            if key_attrs:
                parts.append(". ".join(key_attrs))

        return ". ".join(parts)

    def _clean_value(self, value: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤."""
        if not value:
            return ""
        # –£–±–∏—Ä–∞–µ–º –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        value = re.sub(r'[‚â•‚â§<>]=?', '', value).strip()
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        value = ' '.join(value.split())
        return value

    def filter_by_similarity(self, tender: Dict[str, Any],
                           products: List[Dict[str, Any]],
                           threshold: float = 0.35) -> Dict[str, Any]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏."""
        if not products:
            return {'products': [], 'stats': {}}

        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —Ç–µ–Ω–¥–µ—Ä–∞
        tender_text = self.create_tender_text(tender)

        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç—ã —Ç–æ–≤–∞—Ä–æ–≤
        product_texts = []
        valid_products = []

        for product in products:
            try:
                text = self.create_product_text(product)
                if text:
                    product_texts.append(text)
                    valid_products.append(product)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–≤–∞—Ä–∞: {e}")
                continue

        if not product_texts:
            return {'products': [], 'stats': {}}

        # –í—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        print(f"üß† –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(product_texts)} —Ç–æ–≤–∞—Ä–æ–≤...")

        with torch.no_grad():
            tender_embedding = self.model.encode([tender_text], convert_to_numpy=True)

            # –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            batch_size = 64
            product_embeddings = []

            for i in range(0, len(product_texts), batch_size):
                batch_texts = product_texts[i:i + batch_size]
                batch_embeddings = self.model.encode(batch_texts, convert_to_numpy=True)
                product_embeddings.append(batch_embeddings)

            product_embeddings = np.vstack(product_embeddings)

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        similarities = cosine_similarity(tender_embedding, product_embeddings)[0]

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        filtered_products = []
        stats = {
            'total_processed': len(valid_products),
            'above_threshold': 0,
            'final_count': 0
        }

        for product, similarity in zip(valid_products, similarities):
            if similarity >= threshold:
                stats['above_threshold'] += 1
                product['semantic_score'] = float(similarity)
                filtered_products.append(product)

        stats['final_count'] = len(filtered_products)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏
        filtered_products.sort(key=lambda x: x['semantic_score'], reverse=True)

        return {
            'products': filtered_products,
            'stats': stats
        }



    def combine_with_es_scores(self, products: List[Dict[str, Any]],
                             es_scores: Dict[str, float]) -> List[Dict[str, Any]]:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ ES –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∫–æ—Ä–æ–≤ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π."""
        for product in products:
            product_id = product['_id']
            es_score = es_scores.get(product_id, 0.0)
            semantic_score = product.get('semantic_score', 0.0)

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º ES —Å–∫–æ—Ä (–æ–±—ã—á–Ω–æ –æ–Ω –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-100)
            normalized_es_score = min(es_score / 10.0, 1.0)

            # –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –µ—Å–ª–∏ ES —Å–∫–æ—Ä –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–π, —Å–Ω–∏–∂–∞–µ–º –≤–ª–∏—è–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏
            if normalized_es_score < 0.05:  # ES —Å–∫–æ—Ä < 0.5
                # ES –ø–æ—á—Ç–∏ –Ω–µ –Ω–∞—à–µ–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π - —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –º–æ–∂–µ—Ç –æ—à–∏–±–∞—Ç—å—Å—è
                # –î–∞–µ–º 80% –≤–µ—Å–∞ ES —Å–∫–æ—Ä—É, —Ç–æ–ª—å–∫–æ 20% —Å–µ–º–∞–Ω—Ç–∏–∫–µ
                final_score = (0.8 * normalized_es_score + 0.2 * semantic_score)
            elif normalized_es_score < 0.2:  # ES —Å–∫–æ—Ä < 2.0
                # –ù–∏–∑–∫–∏–π ES —Å–∫–æ—Ä - –æ—Å—Ç–æ—Ä–æ–∂–Ω–µ–µ —Å —Å–µ–º–∞–Ω—Ç–∏–∫–æ–π
                final_score = (0.65 * normalized_es_score + 0.35 * semantic_score)
            elif semantic_score > 0.75 and normalized_es_score < 0.3:
                # –í—ã—Å–æ–∫–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ + –Ω–∏–∑–∫–∏–π ES = –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ
                final_score = (0.6 * normalized_es_score + 0.4 * semantic_score)
            else:
                # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è - —Ä–∞–≤–Ω—ã–µ –≤–µ—Å–∞
                final_score = (0.5 * normalized_es_score + 0.5 * semantic_score)

            product['es_score'] = normalized_es_score
            product['final_score'] = final_score

        return products
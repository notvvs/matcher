import os

os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer, CrossEncoder
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct
from pymongo import MongoClient
from bson import ObjectId
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field
import re
from collections import defaultdict
import logging
from tqdm import tqdm

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MONGODB_URI = "mongodb://localhost:27017/"
DATABASE_NAME = "all_products"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à—É –ë–î
COLLECTION_NAME = "products"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à—É –∫–æ–ª–ª–µ–∫—Ü–∏—é

ELASTICSEARCH_HOST = "http://localhost:9200"
ES_INDEX_NAME = "all_products"

QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
QDRANT_COLLECTION = "all_products"

# –ú–æ–¥–µ–ª–∏
EMBEDDING_MODEL = 'ai-forever/sbert_large_nlu_ru'
CROSS_ENCODER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'  # –ò–ª–∏ —Ä—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å


@dataclass
class SearchConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞"""
    # –í–µ—Å–∞ –¥–ª—è RRF
    elastic_weight: float = 0.5
    vector_weight: float = 0.5

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RRF - –£–ú–ï–ù–¨–®–ê–ï–ú K –¥–ª—è –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏—Ö —Å–∫–æ—Ä–æ–≤
    rrf_k: int = 10  # –ë—ã–ª–æ 60

    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    candidates_multiplier: int = 5  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ElasticSearch
    fuzziness: str = "AUTO"
    prefix_length: int = 2

    # Boosting –¥–ª—è –ø–æ–ª–µ–π –≤ ES
    title_boost: float = 3.0
    description_boost: float = 1.0
    category_boost: float = 1.5
    brand_boost: float = 2.0
    attributes_boost: float = 1.5  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –∞—Ç—Ä–∏–±—É—Ç–æ–≤

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å cross-encoder –¥–ª—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    use_cross_encoder: bool = True
    cross_encoder_top_k: int = 30  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º


@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    id: str
    title: str
    score: float
    elastic_score: float = 0.0
    vector_score: float = 0.0
    cross_encoder_score: float = 0.0
    debug_info: Dict = field(default_factory=dict)


class UniversalHybridSearch:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–∏–±—Ä–∏–¥–Ω–∞—è –ø–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞"""

    def __init__(self, config: SearchConfig = None):
        self.config = config or SearchConfig()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
        try:
            self.es = Elasticsearch([ELASTICSEARCH_HOST])
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            if not self.es.ping():
                raise Exception("ElasticSearch –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç")
            logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ElasticSearch —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ElasticSearch: {e}")
            logger.error(f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ ElasticSearch –∑–∞–ø—É—â–µ–Ω –Ω–∞ {ELASTICSEARCH_HOST}")
            raise

        self.qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        self.mongo = MongoClient(MONGODB_URI)[DATABASE_NAME][COLLECTION_NAME]

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
        self.embedder = SentenceTransformer(EMBEDDING_MODEL)
        if self.config.use_cross_encoder:
            self.cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞/—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
        self._setup_elasticsearch()

    def _parse_query_attributes(self, query: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        attributes = {}

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        patterns = {
            'diameter': r'(\d+(?:\.\d+)?)\s*–º–º',
            'color': r'(—Å–∏–Ω–∏–π|—á–µ—Ä–Ω—ã–π|–∫—Ä–∞—Å–Ω—ã–π|–∑–µ–ª–µ–Ω—ã–π|–∂–µ–ª—Ç—ã–π|–±–µ–ª—ã–π|–±–µ—Å—Ü–≤–µ—Ç–Ω—ã–π|–ø—Ä–æ–∑—Ä–∞—á–Ω—ã–π)',
            'sheets': r'(\d+)\s*(?:–ª–∏—Å—Ç–æ–≤|–ª–∏—Å—Ç|–ª\.)',
            'format': r'(A\d|–ê\d)',
            'quantity': r'(?:–Ω–µ –º–µ–Ω–µ–µ|–æ—Ç|‚â•)\s*(\d+)',
            'type': r'—Ç–∏–ø\s+(\d+|[–∞-—è–ê-–Ø]+)',
        }

        query_lower = query.lower()

        for attr_name, pattern in patterns.items():
            match = re.search(pattern, query_lower, re.IGNORECASE)
            if match:
                attributes[attr_name] = match.group(1)

        return attributes

    def _setup_elasticsearch(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ ElasticSearch —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é ElasticSearch
        try:
            info = self.es.info()
            es_version = info['version']['number']
            logger.info(f"ElasticSearch –≤–µ—Ä—Å–∏—è: {es_version}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ ElasticSearch: {e}")

        # Mapping –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        index_settings = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "filter": {
                        "russian_stop": {
                            "type": "stop",
                            "stopwords": "_russian_"
                        },
                        "russian_stemmer": {
                            "type": "stemmer",
                            "language": "russian"
                        },
                        "edge_ngram_filter": {
                            "type": "edge_ngram",
                            "min_gram": 2,
                            "max_gram": 20
                        }
                    },
                    "analyzer": {
                        "russian_analyzer": {
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "russian_stop",
                                "russian_stemmer"
                            ]
                        },
                        "edge_ngram_analyzer": {
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "edge_ngram_filter"
                            ]
                        },
                        "keyword_analyzer": {
                            "tokenizer": "keyword",
                            "filter": ["lowercase"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "title": {
                        "type": "text",
                        "analyzer": "russian_analyzer",
                        "fields": {
                            "exact": {
                                "type": "keyword"
                            },
                            "ngram": {
                                "type": "text",
                                "analyzer": "edge_ngram_analyzer",
                                "search_analyzer": "standard"
                            }
                        }
                    },
                    "description": {
                        "type": "text",
                        "analyzer": "russian_analyzer"
                    },
                    "category": {
                        "type": "text",
                        "analyzer": "russian_analyzer",
                        "fields": {
                            "keyword": {
                                "type": "keyword"
                            }
                        }
                    },
                    "brand": {
                        "type": "text",
                        "fields": {
                            "keyword": {
                                "type": "keyword"
                            }
                        }
                    },
                    "article": {
                        "type": "keyword"
                    },
                    "attributes": {
                        "type": "nested",
                        "properties": {
                            "attr_name": {"type": "keyword"},
                            "attr_value": {"type": "text", "analyzer": "russian_analyzer"}
                        }
                    },
                    "all_text": {
                        "type": "text",
                        "analyzer": "russian_analyzer"
                    },
                    "min_price": {
                        "type": "float"
                    }
                }
            }
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
            exists = False
            try:
                exists = self.es.indices.exists(index=ES_INDEX_NAME)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∏–Ω–¥–µ–∫—Å–∞: {e}")
                # –ü—ã—Ç–∞–µ–º—Å—è —É–¥–∞–ª–∏—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –∏–Ω–¥–µ–∫—Å
                try:
                    self.es.indices.delete(index=ES_INDEX_NAME)
                    logger.info(f"–£–¥–∞–ª–µ–Ω –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –∏–Ω–¥–µ–∫—Å {ES_INDEX_NAME}")
                except:
                    pass

            if not exists:
                logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ {ES_INDEX_NAME}...")
                self.es.indices.create(index=ES_INDEX_NAME, body=index_settings)
                logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å {ES_INDEX_NAME} —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            else:
                logger.info(f"–ò–Ω–¥–µ–∫—Å {ES_INDEX_NAME} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            try:
                simple_settings = {
                    "settings": {
                        "number_of_shards": 1,
                        "number_of_replicas": 0
                    },
                    "mappings": {
                        "properties": {
                            "title": {"type": "text"},
                            "description": {"type": "text"},
                            "category": {"type": "keyword"},
                            "brand": {"type": "keyword"},
                            "article": {"type": "keyword"},
                            "all_text": {"type": "text"},
                            "min_price": {"type": "float"}
                        }
                    }
                }
                self.es.indices.create(index=ES_INDEX_NAME, body=simple_settings)
                logger.warning("–°–æ–∑–¥–∞–Ω —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤")
            except Exception as e2:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∞–∂–µ —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {e2}")
                raise

    def index_products(self, batch_size: int = 100):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –≤ ElasticSearch –∏ Qdrant"""

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Qdrant
        try:
            collection_info = self.qdrant.get_collection(QDRANT_COLLECTION)
            qdrant_exists = True
            logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è Qdrant —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {collection_info.points_count} —Ç–æ—á–µ–∫")
        except:
            qdrant_exists = False
            logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant...")
            vector_size = self.embedder.get_sentence_embedding_dimension()
            self.qdrant.create_collection(
                collection_name=QDRANT_COLLECTION,
                vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
            )

        # –ü–æ–¥—Å—á–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        total_docs = self.mongo.count_documents({})
        logger.info(f"–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {total_docs}")

        # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞—Ç—á–∞–º–∏
        es_actions = []
        qdrant_points = []
        texts_for_embedding = []

        with tqdm(total=total_docs, desc="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è") as pbar:
            for doc in self.mongo.find().batch_size(batch_size):
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ElasticSearch
                es_doc = self._prepare_es_document(doc)
                es_actions.append({
                    "_index": ES_INDEX_NAME,
                    "_id": str(doc["_id"]),
                    "_source": es_doc
                })

                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                embedding_text = self._prepare_embedding_text(doc)
                texts_for_embedding.append(embedding_text)

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
                if len(es_actions) >= batch_size:
                    # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ ElasticSearch
                    helpers.bulk(self.es, es_actions)

                    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant
                    if not qdrant_exists or True:  # Always update
                        embeddings = self.embedder.encode(
                            texts_for_embedding,
                            normalize_embeddings=True,
                            show_progress_bar=False
                        )

                        for i, (action, embedding) in enumerate(zip(es_actions, embeddings)):
                            qdrant_points.append(PointStruct(
                                id=abs(hash(action["_id"])) % (10 ** 8),
                                vector=embedding.tolist(),
                                payload={
                                    "mongodb_id": action["_id"],
                                    "title": action["_source"]["title"]
                                }
                            ))

                        self.qdrant.upsert(
                            collection_name=QDRANT_COLLECTION,
                            points=qdrant_points
                        )

                    pbar.update(len(es_actions))
                    es_actions = []
                    qdrant_points = []
                    texts_for_embedding = []

            # –ü–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á
            if es_actions:
                helpers.bulk(self.es, es_actions)

                if not qdrant_exists or True:
                    embeddings = self.embedder.encode(
                        texts_for_embedding,
                        normalize_embeddings=True,
                        show_progress_bar=False
                    )

                    for i, (action, embedding) in enumerate(zip(es_actions, embeddings)):
                        qdrant_points.append(PointStruct(
                            id=abs(hash(action["_id"])) % (10 ** 8),
                            vector=embedding.tolist(),
                            payload={
                                "mongodb_id": action["_id"],
                                "title": action["_source"]["title"]
                            }
                        ))

                    self.qdrant.upsert(
                        collection_name=QDRANT_COLLECTION,
                        points=qdrant_points
                    )

                pbar.update(len(es_actions))

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        self.es.indices.refresh(index=ES_INDEX_NAME)
        logger.info("–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

    def _prepare_es_document(self, mongo_doc: Dict) -> Dict:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è ElasticSearch"""

        # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
        es_doc = {
            "title": mongo_doc.get("title", ""),
            "description": mongo_doc.get("description", ""),
            "category": mongo_doc.get("category", ""),
            "brand": mongo_doc.get("brand", ""),
            "article": mongo_doc.get("article", "")
        }

        # –ê—Ç—Ä–∏–±—É—Ç—ã
        if "attributes" in mongo_doc:
            es_doc["attributes"] = mongo_doc["attributes"]

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞
        min_price = self._extract_min_price(mongo_doc)
        if min_price:
            es_doc["min_price"] = min_price

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
        all_text_parts = [
            es_doc["title"],
            es_doc["description"],
            es_doc["category"],
            es_doc["brand"]
        ]

        # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –≤ –æ–±—â–∏–π —Ç–µ–∫—Å—Ç
        if "attributes" in mongo_doc:
            for attr in mongo_doc["attributes"]:
                if attr.get("attr_value") and attr["attr_value"] != "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
                    all_text_parts.append(f"{attr.get('attr_name', '')}: {attr['attr_value']}")

        es_doc["all_text"] = " ".join(filter(None, all_text_parts))

        return es_doc

    def _prepare_embedding_text(self, mongo_doc: Dict) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –≤–∞–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã"""
        parts = []

        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è —Å –≤–µ—Å–∞–º–∏ (–ø–æ–≤—Ç–æ—Ä—è–µ–º –≤–∞–∂–Ω—ã–µ)
        if mongo_doc.get("title"):
            # –ù–∞–∑–≤–∞–Ω–∏–µ - —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ, –ø–æ–≤—Ç–æ—Ä—è–µ–º 3 —Ä–∞–∑–∞
            parts.extend([mongo_doc["title"]] * 3)

        if mongo_doc.get("description"):
            parts.append(mongo_doc["description"])

        if mongo_doc.get("category") and mongo_doc["category"] != "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
            parts.append(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {mongo_doc['category']}")
            parts.append(mongo_doc['category'])  # –î—É–±–ª–∏—Ä—É–µ–º –¥–ª—è –≤–∞–∂–Ω–æ—Å—Ç–∏

        if mongo_doc.get("brand") and mongo_doc["brand"] != "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
            parts.append(f"–ë—Ä–µ–Ω–¥: {mongo_doc['brand']}")
            parts.append(mongo_doc['brand'])  # –î—É–±–ª–∏—Ä—É–µ–º

        # –í–∞–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã —Å —É—Å–∏–ª–µ–Ω–∏–µ–º
        if "attributes" in mongo_doc:
            important_attrs = []
            for attr in mongo_doc["attributes"]:
                if attr.get("attr_value") and attr["attr_value"] != "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
                    attr_text = f"{attr.get('attr_name', '')}: {attr['attr_value']}"
                    parts.append(attr_text)

                    # –û—Å–æ–±–æ –≤–∞–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥—É–±–ª–∏—Ä—É–µ–º
                    attr_name_lower = attr.get('attr_name', '').lower()
                    if any(keyword in attr_name_lower for keyword in
                           ['—Ü–≤–µ—Ç', '—Ä–∞–∑–º–µ—Ä', '–¥–∏–∞–º–µ—Ç—Ä', '—Ñ–æ—Ä–º–∞—Ç', '—Ç–∏–ø', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ']):
                        important_attrs.append(attr_text)

            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –µ—â–µ —Ä–∞–∑
            parts.extend(important_attrs)

        # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ä—Ç–∏–∫—É–ª –µ—Å–ª–∏ –µ—Å—Ç—å
        if mongo_doc.get("article") and mongo_doc["article"] != "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
            parts.append(f"–ê—Ä—Ç–∏–∫—É–ª: {mongo_doc['article']}")

        return " ".join(parts)

    def _extract_min_price(self, mongo_doc: Dict) -> Optional[float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ü–µ–Ω—ã"""
        min_price = None

        for supplier in mongo_doc.get("suppliers", []):
            for offer in supplier.get("supplier_offers", []):
                for price_item in offer.get("price", []):
                    if "price" in price_item and isinstance(price_item["price"], (int, float)):
                        if min_price is None or price_item["price"] < min_price:
                            min_price = float(price_item["price"])

        return min_price

    def search(self, query: str, top_k: int = 10, filters: Dict = None) -> List[SearchResult]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞"""

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
        es_results = self._elasticsearch_search(query, top_k * self.config.candidates_multiplier, filters)
        vector_results = self._vector_search(query, top_k * self.config.candidates_multiplier)

        # RRF –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        combined_results = self._reciprocal_rank_fusion(es_results, vector_results)

        # Cross-encoder –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if self.config.use_cross_encoder and len(combined_results) > 0:
            combined_results = self._cross_encoder_rerank(query, combined_results[:self.config.cross_encoder_top_k])

        return combined_results[:top_k]

    def _elasticsearch_search(self, query: str, size: int, filters: Dict = None) -> List[Tuple[str, float]]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ ElasticSearch —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∞—Ç—Ä–∏–±—É—Ç–æ–≤"""

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query_attributes = self._parse_query_attributes(query)

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_lower = query.lower()

        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å - –∞—Ä—Ç–∏–∫—É–ª
        if re.match(r'^[A-Za-z0-9\-]+$', query) and len(query) > 4:
            es_query = {
                "bool": {
                    "should": [
                        {"term": {"article": query}},
                        {"match": {"article": {"query": query, "fuzziness": "AUTO"}}}
                    ]
                }
            }
        else:
            # –°–æ–∑–¥–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å
            should_clauses = [
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã
                {
                    "match_phrase": {
                        "title": {
                            "query": query,
                            "boost": self.config.title_boost * 2
                        }
                    }
                },
                # –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Å–ª–æ–≤–∞–º
                {
                    "multi_match": {
                        "query": query,
                        "fields": [
                            f"title^{self.config.title_boost}",
                            f"description^{self.config.description_boost}",
                            f"category^{self.config.category_boost}",
                            f"brand^{self.config.brand_boost}",
                            "all_text"
                        ],
                        "type": "best_fields",
                        "fuzziness": self.config.fuzziness,
                        "prefix_length": self.config.prefix_length
                    }
                }
            ]

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∞—Ç—Ä–∏–±—É—Ç–∞–º –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã
            if query_attributes:
                # –î–ª—è —Ü–≤–µ—Ç–∞
                if 'color' in query_attributes:
                    color = query_attributes['color']
                    should_clauses.append({
                        "nested": {
                            "path": "attributes",
                            "query": {
                                "bool": {
                                    "should": [
                                        {"match": {"attributes.attr_value": color}},
                                        {"match": {"description": color}},
                                        {"match": {"title": color}}
                                    ]
                                }
                            },
                            "boost": 2.0
                        }
                    })

                # –î–ª—è –¥–∏–∞–º–µ—Ç—Ä–∞/—Ä–∞–∑–º–µ—Ä–∞
                if 'diameter' in query_attributes:
                    diameter = query_attributes['diameter']
                    should_clauses.extend([
                        {"match": {"title": f"{diameter}–º–º"}},
                        {"match": {"description": f"{diameter}–º–º"}},
                        {
                            "nested": {
                                "path": "attributes",
                                "query": {
                                    "match": {"attributes.attr_value": f"{diameter}"}
                                },
                                "boost": 1.5
                            }
                        }
                    ])

                # –î–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∏—Å—Ç–æ–≤
                if 'sheets' in query_attributes:
                    sheets = query_attributes['sheets']
                    should_clauses.extend([
                        {"match": {"title": f"{sheets} –ª–∏—Å—Ç"}},
                        {"match": {"description": f"{sheets} –ª–∏—Å—Ç"}},
                        {
                            "nested": {
                                "path": "attributes",
                                "query": {
                                    "match": {"attributes.attr_value": sheets}
                                },
                                "boost": 1.5
                            }
                        }
                    ])

                # –î–ª—è —Ñ–æ—Ä–º–∞—Ç–∞
                if 'format' in query_attributes:
                    format_value = query_attributes['format'].upper()
                    should_clauses.extend([
                        {"match": {"title": format_value}},
                        {"match": {"description": format_value}},
                        {
                            "nested": {
                                "path": "attributes",
                                "query": {
                                    "match": {"attributes.attr_value": format_value}
                                },
                                "boost": 2.0
                            }
                        }
                    ])

            es_query = {
                "bool": {
                    "should": should_clauses,
                    "minimum_should_match": 1
                }
            }

            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
            negative_patterns = self._detect_negative_patterns(query)
            if negative_patterns:
                es_query["bool"]["must_not"] = [
                    {"match": {"title": pattern}} for pattern in negative_patterns
                ]

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã
        if filters:
            if "bool" not in es_query:
                es_query = {"bool": {"must": [es_query]}}

            filter_clauses = []

            if "category" in filters:
                filter_clauses.append({"term": {"category.keyword": filters["category"]}})

            if "price_range" in filters:
                filter_clauses.append({
                    "range": {
                        "min_price": {
                            "gte": filters["price_range"][0],
                            "lte": filters["price_range"][1]
                        }
                    }
                })

            if filter_clauses:
                es_query["bool"]["filter"] = filter_clauses

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        response = self.es.search(
            index=ES_INDEX_NAME,
            body={
                "query": es_query,
                "size": size,
                "_source": ["title", "category", "brand"],
                "highlight": {
                    "fields": {
                        "title": {},
                        "description": {},
                        "attributes.attr_value": {}
                    }
                }
            }
        )

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        logger.debug(f"ES –∑–∞–ø—Ä–æ—Å –¥–ª—è '{query}': –Ω–∞–π–¥–µ–Ω–æ {response['hits']['total']['value']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        if query_attributes:
            logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã: {query_attributes}")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ (id, score) —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Å–∫–æ—Ä–æ–≤
        results = []
        max_score = response['hits']['max_score'] or 1.0

        for hit in response["hits"]["hits"]:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–∫–æ—Ä –æ—Ç 0 –¥–æ 1
            normalized_score = hit["_score"] / max_score if max_score > 0 else 0
            results.append((hit["_id"], normalized_score))

        return results

    def _vector_search(self, query: str, limit: int) -> List[Tuple[str, float]]:
        """–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Qdrant"""

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embedder.encode(query, normalize_embeddings=True)

        # –ü–æ–∏—Å–∫
        try:
            results = self.qdrant.query_points(
                collection_name=QDRANT_COLLECTION,
                query=query_embedding.tolist(),
                limit=limit
            ).points
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ (id, score)
        return [(point.payload["mongodb_id"], point.score) for point in results]

    def _reciprocal_rank_fusion(self,
                                es_results: List[Tuple[str, float]],
                                vector_results: List[Tuple[str, float]]) -> List[SearchResult]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ RRF —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–æ—Ä–º—É–ª–æ–π"""

        from bson import ObjectId

        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫–æ—Ä–æ–≤
        all_scores = defaultdict(lambda: {
            "es_rank": None,
            "vector_rank": None,
            "es_score": 0,
            "vector_score": 0,
            "es_normalized": 0,
            "vector_normalized": 0
        })

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫–æ—Ä–æ–≤ ElasticSearch
        if es_results:
            max_es_score = max(score for _, score in es_results[:10])  # –¢–æ–ø-10 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            for rank, (doc_id, score) in enumerate(es_results):
                all_scores[doc_id]["es_rank"] = rank + 1
                all_scores[doc_id]["es_score"] = score
                all_scores[doc_id]["es_normalized"] = score / max_es_score if max_es_score > 0 else 0

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—Å–∫–æ—Ä—ã —É–∂–µ –æ—Ç 0 –¥–æ 1)
        for rank, (doc_id, score) in enumerate(vector_results):
            all_scores[doc_id]["vector_rank"] = rank + 1
            all_scores[doc_id]["vector_score"] = score
            all_scores[doc_id]["vector_normalized"] = score

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–∫–æ—Ä–æ–≤
        final_results = []

        for doc_id, scores in all_scores.items():
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
            # 1. RRF –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
            rrf_score = 0
            if scores["es_rank"] is not None:
                rrf_score += self.config.elastic_weight / (self.config.rrf_k + scores["es_rank"])
            if scores["vector_rank"] is not None:
                rrf_score += self.config.vector_weight / (self.config.rrf_k + scores["vector_rank"])

            # 2. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–æ—Ä—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
            norm_score = (
                    self.config.elastic_weight * scores["es_normalized"] +
                    self.config.vector_weight * scores["vector_normalized"]
            )

            # 3. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
            # –î–∞–µ–º –±–æ–ª—å—à–µ –≤–µ—Å–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º —Å–∫–æ—Ä–∞–º –¥–ª—è –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            final_score = 0.3 * rrf_score + 0.7 * norm_score

            # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –≤ –æ–±–æ–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
            if scores["es_rank"] is not None and scores["vector_rank"] is not None:
                final_score *= 1.1

            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            doc = None

            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ MongoDB –ø–æ ObjectId
            try:
                doc = self.mongo.find_one({"_id": ObjectId(doc_id)})
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å {doc_id} –≤ ObjectId: {e}")

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ MongoDB, –±–µ—Ä–µ–º –∏–∑ ElasticSearch
            if not doc:
                try:
                    es_doc = self.es.get(index=ES_INDEX_NAME, id=doc_id)
                    if es_doc and '_source' in es_doc:
                        doc = es_doc['_source']
                        doc['_id'] = doc_id
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç {doc_id} –∏–∑ ES: {e}")
                    continue

            if doc:
                final_results.append(SearchResult(
                    id=doc_id,
                    title=doc.get("title", ""),
                    score=final_score,
                    elastic_score=scores["es_normalized"],
                    vector_score=scores["vector_normalized"],
                    debug_info={
                        "es_rank": scores["es_rank"],
                        "vector_rank": scores["vector_rank"],
                        "rrf_score": rrf_score,
                        "norm_score": norm_score
                    }
                ))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É —Å–∫–æ—Ä—É
        final_results.sort(key=lambda x: x.score, reverse=True)

        return final_results

    def _cross_encoder_rerank(self, query: str, results: List[SearchResult]) -> List[SearchResult]:
        """–ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é cross-encoder"""
        from bson import ObjectId

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä –¥–ª—è cross-encoder
        pairs = []
        valid_results = []

        for result in results:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            doc = None
            try:
                doc = self.mongo.find_one({"_id": ObjectId(result.id)})
            except:
                pass

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ MongoDB, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ ElasticSearch
            if not doc:
                try:
                    es_doc = self.es.get(index=ES_INDEX_NAME, id=result.id)
                    if es_doc and '_source' in es_doc:
                        doc = es_doc['_source']
                except:
                    pass

            if doc:
                # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                doc_parts = [doc.get('title', '')]

                if doc.get('description'):
                    doc_parts.append(doc['description'])

                if doc.get('category'):
                    doc_parts.append(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {doc['category']}")

                if doc.get('brand') and doc['brand'] != '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö':
                    doc_parts.append(f"–ë—Ä–µ–Ω–¥: {doc['brand']}")

                # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                if 'attributes' in doc:
                    for attr in doc['attributes']:
                        if attr.get('attr_value') and attr['attr_value'] != '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö':
                            doc_parts.append(f"{attr.get('attr_name', '')}: {attr['attr_value']}")

                doc_text = " ".join(doc_parts)
                pairs.append([query, doc_text])
                valid_results.append(result)
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
                pairs.append([query, result.title])
                valid_results.append(result)

        # –ü–æ–ª—É—á–∞–µ–º —Å–∫–æ—Ä—ã –æ—Ç cross-encoder
        if pairs:
            ce_scores = self.cross_encoder.predict(pairs)

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º CE —Å–∫–æ—Ä—ã
            ce_scores_normalized = []
            ce_min = min(ce_scores)
            ce_max = max(ce_scores)
            ce_range = ce_max - ce_min

            if ce_range > 0:
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç 0 –¥–æ 1
                for score in ce_scores:
                    normalized = (score - ce_min) / ce_range
                    ce_scores_normalized.append(normalized)
            else:
                ce_scores_normalized = [0.5] * len(ce_scores)

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result, ce_score, ce_norm in zip(valid_results, ce_scores, ce_scores_normalized):
                result.cross_encoder_score = float(ce_score)

                # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
                # –î–∞–µ–º –±–æ–ª—å—à–µ –≤–µ—Å–∞ cross-encoder –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                result.score = (
                        0.4 * result.score +  # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä (RRF + normalized)
                        0.6 * ce_norm  # Cross-encoder —Å–∫–æ—Ä
                )

                # –ë–æ–Ω—É—Å –∑–∞ –≤—ã—Å–æ–∫–∏–π CE —Å–∫–æ—Ä
                if ce_norm > 0.8:
                    result.score *= 1.1
                elif ce_norm < 0.2:
                    result.score *= 0.9

        # –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        valid_results.sort(key=lambda x: x.score, reverse=True)

        return valid_results

    def _detect_negative_patterns(self, query: str) -> List[str]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        negative_patterns = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ—Å—Ç–µ–π
        opposite_prefixes = ["–∞–Ω—Ç–∏", "–¥–µ", "—Ä–∞–∑", "–±–µ–∑"]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∑–∞–ø—Ä–æ—Å –±–∞–∑–æ–≤–æ–µ —Å–ª–æ–≤–æ –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞
        words = query.lower().split()
        for word in words:
            # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –ù–ï –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞
            has_negative_prefix = any(word.startswith(prefix) for prefix in opposite_prefixes)

            if not has_negative_prefix and len(word) > 4:
                # –î–æ–±–∞–≤–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤ —Ñ–∏–ª—å—Ç—Ä
                for prefix in opposite_prefixes:
                    negative_patterns.append(f"{prefix}{word}")

        return negative_patterns


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def test_search_system():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã"""

    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞
    search_system = UniversalHybridSearch()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–∞ –ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    doc_count = search_system.es.count(index=ES_INDEX_NAME)["count"]
    if doc_count == 0:
        logger.info("–ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç, –∑–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é...")
        search_system.index_products()
    else:
        logger.info(f"–ò–Ω–¥–µ–∫—Å —Å–æ–¥–µ—Ä–∂–∏—Ç {doc_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "—Å—Ç–µ–ø–ª–µ—Ä",
        "–∞–Ω—Ç–∏—Å—Ç–µ–ø–ª–µ—Ä",
        "–Ω–æ—É—Ç–±—É–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã",
        "—Å–æ–¥–∞ –ø–∏—â–µ–≤–∞—è",
        "046727",  # –∞—Ä—Ç–∏–∫—É–ª
        "—Ä—É—á–∫–∞ —à–∞—Ä–∏–∫–æ–≤–∞—è 0.5–º–º —Å–∏–Ω—è—è",
        "—Ñ–æ—Ç–æ–±—É–º–∞–≥–∞ –ê4 50 –ª–∏—Å—Ç–æ–≤",
    ]

    for query in test_queries:
        print(f"\n{'=' * 60}")
        print(f"–ó–∞–ø—Ä–æ—Å: {query}")
        print('=' * 60)

        results = search_system.search(query, top_k=5)

        for i, result in enumerate(results, 1):
            print(f"\n{i}. {result.title}")
            print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä: {result.score:.4f}")
            print(f"   ES —Å–∫–æ—Ä: {result.elastic_score:.4f} (—Ä–∞–Ω–≥: {result.debug_info.get('es_rank', 'N/A')})")
            print(f"   –í–µ–∫—Ç–æ—Ä–Ω—ã–π —Å–∫–æ—Ä: {result.vector_score:.4f} (—Ä–∞–Ω–≥: {result.debug_info.get('vector_rank', 'N/A')})")
            if result.cross_encoder_score > 0:
                print(f"   Cross-encoder —Å–∫–æ—Ä: {result.cross_encoder_score:.4f}")


# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
def interactive_search():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫"""
    from bson import ObjectId

    search_system = UniversalHybridSearch()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    doc_count = search_system.es.count(index=ES_INDEX_NAME)["count"]
    if doc_count == 0:
        print("‚ùå –ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç! –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.")
        response = input("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å–µ–π—á–∞—Å? (y/n): ")
        if response.lower() == 'y':
            search_system.index_products()
        else:
            return

    print("\n" + "=" * 60)
    print("–£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ì–ò–ë–†–ò–î–ù–´–ô –ü–û–ò–°–ö")
    print("=" * 60)
    print("–ö–æ–º–∞–Ω–¥—ã:")
    print("  'exit' - –≤—ã—Ö–æ–¥")
    print("  'debug on/off' - –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –æ—Ç–ª–∞–¥–∫—É")
    print("  'config' - –ø–æ–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")
    print("  'set k VALUE' - –∏–∑–º–µ–Ω–∏—Ç—å RRF K (—Ç–µ–∫—É—â–µ–µ: 10)")
    print("  'set weights ES_WEIGHT VECTOR_WEIGHT' - –∏–∑–º–µ–Ω–∏—Ç—å –≤–µ—Å–∞")
    print("  'reindex' - –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")
    print("=" * 60)

    debug_mode = False

    while True:
        query = input("\nüîç –í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å: ").strip()

        if query.lower() == 'exit':
            break

        if query.lower() == 'debug on':
            debug_mode = True
            print("‚úÖ –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ –≤–∫–ª—é—á–µ–Ω")
            continue

        if query.lower() == 'debug off':
            debug_mode = False
            print("‚úÖ –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ –≤—ã–∫–ª—é—á–µ–Ω")
            continue

        if query.lower() == 'config':
            print("\n–¢–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
            print(f"  ES –≤–µ—Å: {search_system.config.elastic_weight}")
            print(f"  –í–µ–∫—Ç–æ—Ä–Ω—ã–π –≤–µ—Å: {search_system.config.vector_weight}")
            print(f"  RRF K: {search_system.config.rrf_k}")
            print(f"  Cross-encoder: {'–í–∫–ª—é—á–µ–Ω' if search_system.config.use_cross_encoder else '–í—ã–∫–ª—é—á–µ–Ω'}")
            continue

        if query.lower().startswith('set k '):
            try:
                new_k = int(query.split()[2])
                search_system.config.rrf_k = new_k
                print(f"‚úÖ RRF K –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ {new_k}")
            except:
                print("‚ùå –§–æ—Ä–º–∞—Ç: set k –ß–ò–°–õ–û")
            continue

        if query.lower().startswith('set weights '):
            try:
                parts = query.split()
                es_weight = float(parts[2])
                vector_weight = float(parts[3])
                search_system.config.elastic_weight = es_weight
                search_system.config.vector_weight = vector_weight
                print(f"‚úÖ –í–µ—Å–∞ –∏–∑–º–µ–Ω–µ–Ω—ã: ES={es_weight}, Vector={vector_weight}")
            except:
                print("‚ùå –§–æ—Ä–º–∞—Ç: set weights ES_WEIGHT VECTOR_WEIGHT")
            continue

        if query.lower() == 'reindex':
            print("–ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
            search_system.index_products()
            continue

        if not query:
            continue

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        try:
            # –ï—Å–ª–∏ —Ä–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            if debug_mode:
                attributes = search_system._parse_query_attributes(query)
                if attributes:
                    print(f"\nüîç –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã:")
                    for key, value in attributes.items():
                        print(f"   - {key}: {value}")

            results = search_system.search(query, top_k=10)

            if results:
                print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
                for i, result in enumerate(results, 1):
                    print(f"\n{i}. {result.title}")
                    print(f"   –°–∫–æ—Ä: {result.score:.4f}")

                    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    if debug_mode:
                        print(f"   –î–µ—Ç–∞–ª–∏ —Å–∫–æ—Ä–∏–Ω–≥–∞:")
                        print(
                            f"     - ES —Å–∫–æ—Ä: {result.elastic_score:.3f} (—Ä–∞–Ω–≥: {result.debug_info.get('es_rank', 'N/A')})")
                        print(
                            f"     - Vector —Å–∫–æ—Ä: {result.vector_score:.3f} (—Ä–∞–Ω–≥: {result.debug_info.get('vector_rank', 'N/A')})")
                        if result.cross_encoder_score > 0:
                            print(f"     - Cross-encoder: {result.cross_encoder_score:.3f}")
                        print(f"     - RRF –∫–æ–º–ø–æ–Ω–µ–Ω—Ç: {result.debug_info.get('rrf_score', 0):.3f}")
                        print(f"     - Norm –∫–æ–º–ø–æ–Ω–µ–Ω—Ç: {result.debug_info.get('norm_score', 0):.3f}")

                    # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    doc = None
                    try:
                        doc = search_system.mongo.find_one({"_id": ObjectId(result.id)})
                    except:
                        pass

                    if not doc:
                        try:
                            es_doc = search_system.es.get(index=ES_INDEX_NAME, id=result.id)
                            if es_doc and '_source' in es_doc:
                                doc = es_doc['_source']
                        except:
                            pass

                    if doc:
                        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {doc.get('category', 'N/A')}")
                        print(f"   –ë—Ä–µ–Ω–¥: {doc.get('brand', 'N/A')}")
                        print(f"   –ê—Ä—Ç–∏–∫—É–ª: {doc.get('article', 'N/A')}")

                        # –¶–µ–Ω–∞
                        min_price = search_system._extract_min_price(doc)
                        if min_price:
                            print(f"   –¶–µ–Ω–∞ –æ—Ç: {min_price} —Ä—É–±.")
            else:
                print("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            logger.error(f"Search error: {e}", exc_info=True)


if __name__ == "__main__":
    print("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:")
    print("1. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ")
    print("2. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫")
    print("3. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")

    choice = input("–í–∞—à –≤—ã–±–æ—Ä (1/2/3): ").strip()

    if choice == "1":
        test_search_system()
    elif choice == "2":
        interactive_search()
    elif choice == "3":
        search_system = UniversalHybridSearch()
        search_system.index_products()
    else:
        print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
import os
import re
from collections import defaultdict

from config.settings import settings


class ConfigurableTermExtractor:
    """–≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–æ–≤"""

    def __init__(self, config_dir=None):
        self.config_dir = config_dir or settings.CONFIG_DIR

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.stop_words = self._load_config_file(settings.STOPWORDS_FILE)
        self.important_chars = self._load_config_file(settings.IMPORTANT_CHARS_FILE)
        self.synonyms_dict = self._load_synonyms()

        print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞:")
        print(f"   - –°—Ç–æ–ø-—Å–ª–æ–≤: {len(self.stop_words)}")
        print(f"   - –í–∞–∂–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(self.important_chars)}")
        print(f"   - –°–∏–Ω–æ–Ω–∏–º–æ–≤: {len(self.synonyms_dict)}")

    def _load_config_file(self, filename):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª"""
        filepath = os.path.join(self.config_dir, filename)
        config_set = set()

        if not os.path.exists(filepath):
            print(f"‚ö†Ô∏è –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return config_set

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        config_set.add(line.lower())

            print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω {filename}: {len(config_set)} –∑–∞–ø–∏—Å–µ–π")
            return config_set

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
            return config_set

    def _load_synonyms(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã"""
        filepath = os.path.join(self.config_dir, settings.SYNONYMS_FILE)
        synonyms_dict = {}

        if not os.path.exists(filepath):
            print(f"‚ö†Ô∏è –§–∞–π–ª —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return synonyms_dict

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and ',' in line:
                        synonyms = [s.strip().lower() for s in line.split(',')]
                        for word in synonyms:
                            if word not in synonyms_dict:
                                synonyms_dict[word] = set()
                            synonyms_dict[word].update(synonyms)
                            synonyms_dict[word].discard(word)

            return synonyms_dict

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤: {e}")
            return {}

    def is_stop_word(self, word):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–æ"""
        return word.lower() in self.stop_words

    def is_important_characteristic(self, word):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–∂–Ω—É—é —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É"""
        word_lower = word.lower()
        return any(important in word_lower for important in self.important_chars)

    def get_synonyms(self, word):
        """–ü–æ–ª—É—á–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã"""
        return list(self.synonyms_dict.get(word.lower(), []))

    def expand_with_synonyms(self, words):
        """–†–∞—Å—à–∏—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
        expanded = set(words)
        for word in words:
            synonyms = self.get_synonyms(word)
            expanded.update(synonyms)
        return list(expanded)

    def extract_from_tender(self, tender_item):
        """–ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""

        tender_name = tender_item.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
        print(f"üéØ –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ô –∞–Ω–∞–ª–∏–∑ —Ç–µ–Ω–¥–µ—Ä–∞: {tender_name}")
        print("-" * 60)

        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—ã—Ä—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        raw_terms = self._extract_raw_terms(tender_item)

        # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º
        classified = self._classify_terms(raw_terms)

        # 3. –†–∞—Å—à–∏—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        expanded = self._expand_classified_terms(classified)

        # 4. –ü–†–ò–ú–ï–ù–Ø–ï–ú –û–ü–¢–ò–ú–ê–õ–¨–ù–£–Æ –õ–û–ì–ò–ö–£ –í–ï–°–û–í
        result = self._build_optimal_tender_weights(expanded, tender_item)

        return result

    def _extract_raw_terms(self, tender_item):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Å—ã—Ä—ã–µ —Ç–µ—Ä–º–∏–Ω—ã"""
        terms = {
            'name_terms': [],
            'char_names': [],
            'char_values': [],
            'all_text': ''
        }

        # –ò–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
        tender_name = tender_item.get('name', '')
        if tender_name:
            terms['name_terms'] = self._clean_and_filter_words(tender_name)
            terms['all_text'] += f" {tender_name}"
            print(f"üìù –ò–∑ –Ω–∞–∑–≤–∞–Ω–∏—è: {terms['name_terms']}")

        # –ò–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        if 'characteristics' in tender_item:
            for char in tender_item['characteristics']:
                char_name = char.get('name', '')
                char_value = char.get('value', '')

                terms['all_text'] += f" {char_name} {char_value}"

                if char_name:
                    char_name_words = self._clean_and_filter_words(char_name)
                    terms['char_names'].extend(char_name_words)

                if char_value and not self._is_range_value(char_value):
                    char_value_words = self._clean_and_filter_words(str(char_value))
                    terms['char_values'].extend(char_value_words)

        print(f"üîç –°—ã—Ä—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: –Ω–∞–∑–≤–∞–Ω–∏–µ={len(terms['name_terms'])}, "
              f"—Ö–∞—Ä-–∫–∏={len(terms['char_names'])}, –∑–Ω–∞—á–µ–Ω–∏—è={len(terms['char_values'])}")

        return terms

    def _clean_and_filter_words(self, text):
        """–û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è"""
        if not text:
            return []

        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text_clean.split()

        filtered = []
        for word in words:
            if (len(word) > 2 and
                    not self.is_stop_word(word) and
                    word.isalpha() and
                    not word.isdigit()):
                filtered.append(word)

        return filtered

    def _is_range_value(self, value):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã"""
        value_str = str(value)
        range_indicators = ['‚â•', '‚â§', '>', '<', '–±–æ–ª–µ–µ', '–º–µ–Ω–µ–µ', '–æ—Ç', '–¥–æ']
        return any(indicator in value_str for indicator in range_indicators)

    def _classify_terms(self, raw_terms):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"""
        classified = {
            'primary': raw_terms['name_terms'][:3],
            'secondary': [],
            'tertiary': []
        }

        for value in raw_terms['char_values']:
            if value not in ['–Ω–µ—Ç', '–¥–∞', '–±–µ–∑', '–Ω–∞–ª–∏—á–∏–µ', '–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ']:
                classified['secondary'].append(value)

        for char_name in raw_terms['char_names']:
            if self.is_important_characteristic(char_name):
                classified['tertiary'].append(char_name)

        print(f"üìä –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: Primary={len(classified['primary'])}, "
              f"Secondary={len(classified['secondary'])}, Tertiary={len(classified['tertiary'])}")

        return classified

    def _expand_classified_terms(self, classified):
        """–†–∞—Å—à–∏—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
        expanded = {}

        for category, terms in classified.items():
            original_count = len(terms)
            expanded[category] = self.expand_with_synonyms(terms)
            new_count = len(expanded[category])
            print(f"üìà {category}: {original_count} ‚Üí {new_count} (+{new_count - original_count} —Å–∏–Ω–æ–Ω–∏–º–æ–≤)")

        return expanded

    def _build_optimal_tender_weights(self, expanded, tender_item):
        """üéØ –û–ü–¢–ò–ú–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê –í–ï–°–û–í –î–õ–Ø –¢–ï–ù–î–ï–†–û–í"""

        result = {
            'search_query': '',
            'boost_terms': {},
            'must_match_terms': [],
            'all_terms': [],
            'debug_info': {}
        }

        # 1. –û–°–ù–û–í–ù–û–ô –ü–û–ò–°–ö–û–í–´–ô –ó–ê–ü–†–û–° - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ô
        if expanded['primary']:
            result['search_query'] = ' '.join(expanded['primary'][:2])  # –¢–æ–ª—å–∫–æ 2 –≥–ª–∞–≤–Ω—ã—Ö —Å–ª–æ–≤–∞
            result['must_match_terms'] = expanded['primary'][:3]

        # 2. –ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò –¢–ï–ù–î–ï–†–ê
        characteristics = tender_item.get('characteristics', [])
        required_chars = [c for c in characteristics if c.get('required', False)]
        optional_chars = [c for c in characteristics if not c.get('required', False)]

        print(f"üìã –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö={len(required_chars)}, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö={len(optional_chars)}")

        # 3. –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ô –í–ï–°
        required_values = []
        for char in required_chars:
            char_value = char.get('value', '')
            if char_value and not self._is_range_value(char_value):
                clean_values = self._clean_and_filter_words(str(char_value))
                required_values.extend(clean_values)

        # –ó–Ω–∞—á–µ–Ω–∏—è –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–• —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –ø–æ–ª—É—á–∞—é—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
        weights_config = settings.WEIGHTS['required_values']
        for i, term in enumerate(required_values[:weights_config['count']]):
            if term in expanded['secondary']:
                weight = weights_config['start'] - (i * weights_config['step'])
                result['boost_terms'][term] = weight

        # 4. –û–ü–¶–ò–û–ù–ê–õ–¨–ù–´–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò - –í–´–°–û–ö–ò–ô –í–ï–°
        optional_values = []
        for char in optional_chars:
            char_value = char.get('value', '')
            if char_value and not self._is_range_value(char_value):
                clean_values = self._clean_and_filter_words(str(char_value))
                optional_values.extend(clean_values)

        weights_config = settings.WEIGHTS['optional_values']
        for i, term in enumerate(optional_values[:weights_config['count']]):
            if term in expanded['secondary'] and term not in result['boost_terms']:
                weight = weights_config['start'] - (i * weights_config['step'])
                result['boost_terms'][term] = weight

        # 5. –ù–ê–ó–í–ê–ù–ò–Ø –í–ê–ñ–ù–´–• –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö - –°–†–ï–î–ù–ò–ô –í–ï–°
        important_char_names = []
        for char in required_chars[:3]:
            char_name = char.get('name', '')
            if char_name:
                clean_names = self._clean_and_filter_words(char_name)
                important_char_names.extend(clean_names)

        weights_config = settings.WEIGHTS['char_names']
        for i, term in enumerate(important_char_names[:weights_config['count']]):
            if term in expanded['tertiary'] and term not in result['boost_terms']:
                weight = weights_config['start'] - (i * weights_config['step'])
                result['boost_terms'][term] = weight

        # 6. –°–ò–ù–û–ù–ò–ú–´ –ü–û–õ–£–ß–ê–Æ–¢ –ü–û–ù–ò–ñ–ï–ù–ù–´–ô –í–ï–°
        original_terms = set()
        original_terms.update(self._clean_and_filter_words(tender_item.get('name', '')))
        for char in characteristics:
            original_terms.update(self._clean_and_filter_words(char.get('name', '')))
            original_terms.update(self._clean_and_filter_words(str(char.get('value', ''))))

        # –°–Ω–∏–∂–∞–µ–º –≤–µ—Å —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        synonym_count = 0
        synonym_penalty = settings.WEIGHTS['synonym_penalty']
        for term, weight in list(result['boost_terms'].items()):
            if term not in original_terms:  # –≠—Ç–æ —Å–∏–Ω–æ–Ω–∏–º
                result['boost_terms'][term] = round(weight * synonym_penalty, 2)
                synonym_count += 1

        # 7. –ê–ù–¢–ò-–®–£–ú–û–í–´–ï –ú–ï–•–ê–ù–ò–ó–ú–´
        original_count = len(result['boost_terms'])
        result['boost_terms'] = {
            term: weight for term, weight in result['boost_terms'].items()
            if weight >= settings.MIN_WEIGHT_THRESHOLD
        }
        removed_count = original_count - len(result['boost_terms'])

        # 8. –í–°–ï –¢–ï–†–ú–ò–ù–´
        for terms in expanded.values():
            result['all_terms'].extend(terms)
        result['all_terms'] = list(set(result['all_terms']))

        # 9. –û–¢–õ–ê–î–û–ß–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø
        result['debug_info'] = {
            'tender_name': tender_item.get('name', ''),
            'required_characteristics': len(required_chars),
            'optional_characteristics': len(optional_chars),
            'must_match_terms': result['must_match_terms'],
            'boost_terms_count': len(result['boost_terms']),
            'synonyms_penalized': synonym_count,
            'low_weight_removed': removed_count,
            'weight_ranges': {
                'required_values': '4.0 ‚Üí 3.2',
                'optional_values': '2.5 ‚Üí 1.9',
                'char_names': '1.8 ‚Üí 1.2',
                'synonym_penalty': '-30%'
            }
        }

        print(f"üéØ –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
        print(f"   - –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å: '{result['search_query']}'")
        print(f"   - –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {result['must_match_terms']}")
        print(f"   - Boost —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(result['boost_terms'])}")
        print(f"   - –°–∏–Ω–æ–Ω–∏–º–æ–≤ —Å –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–º –≤–µ—Å–æ–º: {synonym_count}")
        print(f"   - –£–±—Ä–∞–Ω–æ —à—É–º–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {removed_count}")

        return result
import os
import re
from collections import defaultdict


class ConfigurableTermExtractor:
    """–≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏"""

    def __init__(self, config_dir="config"):
        self.config_dir = config_dir

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.stop_words = self._load_config_file("C:/Users/ruzik/PycharmProjects/work/matcher/config/stopwords.txt")
        self.important_chars = self._load_config_file("C:/Users/ruzik/PycharmProjects/work/matcher/config/important_chars.txt")
        self.synonyms_dict = self._load_synonyms()

        print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞:")
        print(f"   - –°—Ç–æ–ø-—Å–ª–æ–≤: {len(self.stop_words)}")
        print(f"   - –í–∞–∂–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(self.important_chars)}")
        print(f"   - –°–∏–Ω–æ–Ω–∏–º–æ–≤: {len(self.synonyms_dict)}")

    def _load_config_file(self, filename):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª"""
        filepath = os.path.join(self.config_dir, filename)
        config_set = set()

        if not os.path.exists(filepath):
            print(f"‚ö†Ô∏è –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return config_set

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    if line and not line.startswith('#'):
                        config_set.add(line.lower())

            print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω {filename}: {len(config_set)} –∑–∞–ø–∏—Å–µ–π")
            return config_set

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
            return config_set

    def _load_synonyms(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã"""
        filepath = "C:/Users/ruzik/PycharmProjects/work/matcher/config/synonyms.txt"
        synonyms_dict = {}

        if not os.path.exists(filepath):
            print(f"‚ö†Ô∏è –§–∞–π–ª —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return synonyms_dict

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and ',' in line:
                        synonyms = [s.strip().lower() for s in line.split(',')]
                        # –ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ —Å–≤—è–∑—ã–≤–∞–µ–º —Å–æ –≤—Å–µ–º–∏ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
                        for word in synonyms:
                            if word not in synonyms_dict:
                                synonyms_dict[word] = set()
                            synonyms_dict[word].update(synonyms)
                            synonyms_dict[word].discard(word)  # –£–±–∏—Ä–∞–µ–º —Å–∞–º–æ —Å–ª–æ–≤–æ

            return synonyms_dict

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤: {e}")
            return {}

    def is_stop_word(self, word):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ–º"""
        return word.lower() in self.stop_words

    def is_important_characteristic(self, word):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∞–∂–Ω–∞—è –ª–∏ —ç—Ç–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞"""
        word_lower = word.lower()
        return any(important in word_lower for important in self.important_chars)

    def get_synonyms(self, word):
        """–ü–æ–ª—É—á–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è —Å–ª–æ–≤–∞"""
        return list(self.synonyms_dict.get(word.lower(), []))

    def expand_with_synonyms(self, words):
        """–†–∞—Å—à–∏—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
        expanded = set(words)

        for word in words:
            synonyms = self.get_synonyms(word)
            expanded.update(synonyms)

        return list(expanded)

    def extract_from_tender(self, tender_item):
        """–ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤"""

        tender_name = tender_item.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
        print(f"üîß –ê–Ω–∞–ª–∏–∑ —Ç–µ–Ω–¥–µ—Ä–∞: {tender_name}")
        print("-" * 50)

        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—ã—Ä—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        raw_terms = self._extract_raw_terms(tender_item)

        # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º
        classified = self._classify_terms(raw_terms)

        # 3. –†–∞—Å—à–∏—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        expanded = self._expand_classified_terms(classified)

        # 4. –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = self._build_final_result(expanded, tender_item)

        return result

    def _extract_raw_terms(self, tender_item):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Å—ã—Ä—ã–µ —Ç–µ—Ä–º–∏–Ω—ã"""
        terms = {
            'name_terms': [],
            'char_names': [],
            'char_values': [],
            'all_text': ''
        }

        # –ò–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ–Ω–¥–µ—Ä–∞
        tender_name = tender_item.get('name', '')
        if tender_name:
            terms['name_terms'] = self._clean_and_filter_words(tender_name)
            terms['all_text'] += f" {tender_name}"
            print(f"üìù –ò–∑ –Ω–∞–∑–≤–∞–Ω–∏—è: {terms['name_terms']}")

        # –ò–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        if 'characteristics' in tender_item:
            for char in tender_item['characteristics']:
                char_name = char.get('name', '')
                char_value = char.get('value', '')

                terms['all_text'] += f" {char_name} {char_value}"

                if char_name:
                    char_name_words = self._clean_and_filter_words(char_name)
                    terms['char_names'].extend(char_name_words)

                if char_value and not self._is_range_value(char_value):
                    char_value_words = self._clean_and_filter_words(str(char_value))
                    terms['char_values'].extend(char_value_words)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"üîç –°—ã—Ä—ã–µ —Ç–µ—Ä–º–∏–Ω—ã:")
        print(f"   - –ù–∞–∑–≤–∞–Ω–∏–µ: {len(terms['name_terms'])}")
        print(f"   - –ù–∞–∑–≤–∞–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(terms['char_names'])}")
        print(f"   - –ó–Ω–∞—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(terms['char_values'])}")

        return terms

    def _clean_and_filter_words(self, text):
        """–û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª–æ–≤"""
        if not text:
            return []

        # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text_clean.split()

        # –§–∏–ª—å—Ç—Ä—É–µ–º
        filtered = []
        for word in words:
            if (len(word) > 2 and
                    not self.is_stop_word(word) and
                    word.isalpha() and
                    not word.isdigit()):
                filtered.append(word)

        return filtered

    def _is_range_value(self, value):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã"""
        value_str = str(value)
        range_indicators = ['‚â•', '‚â§', '>', '<', '–±–æ–ª–µ–µ', '–º–µ–Ω–µ–µ', '–æ—Ç', '–¥–æ']
        return any(indicator in value_str for indicator in range_indicators)

    def _classify_terms(self, raw_terms):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏"""
        classified = {
            'primary': raw_terms['name_terms'][:3],  # –ò–∑ –Ω–∞–∑–≤–∞–Ω–∏—è - —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ
            'secondary': [],  # –ó–Ω–∞—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ - –≤–∞–∂–Ω—ã–µ
            'tertiary': []  # –ù–∞–∑–≤–∞–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ - –º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–µ
        }

        # –ó–Ω–∞—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        for value in raw_terms['char_values']:
            if value not in ['–Ω–µ—Ç', '–¥–∞', '–±–µ–∑', '–Ω–∞–ª–∏—á–∏–µ', '–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ']:
                classified['secondary'].append(value)

        # –í–∞–∂–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        for char_name in raw_terms['char_names']:
            if self.is_important_characteristic(char_name):
                classified['tertiary'].append(char_name)

        print(f"üìä –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:")
        print(f"   - Primary (–Ω–∞–∑–≤–∞–Ω–∏–µ): {len(classified['primary'])}")
        print(f"   - Secondary (–∑–Ω–∞—á–µ–Ω–∏—è): {len(classified['secondary'])}")
        print(f"   - Tertiary (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏): {len(classified['tertiary'])}")

        return classified

    def _expand_classified_terms(self, classified):
        """–†–∞—Å—à–∏—Ä—è–µ–º –∫–∞–∂–¥—É—é –≥—Ä—É–ø–ø—É —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
        expanded = {}

        for category, terms in classified.items():
            original_count = len(terms)
            expanded[category] = self.expand_with_synonyms(terms)
            new_count = len(expanded[category])
            print(f"üìà {category}: {original_count} ‚Üí {new_count} (–¥–æ–±–∞–≤–ª–µ–Ω–æ {new_count - original_count} —Å–∏–Ω–æ–Ω–∏–º–æ–≤)")

        return expanded

    def _build_final_result(self, expanded, tender_item):
        """–§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        result = {
            'search_query': '',
            'boost_terms': {},
            'all_terms': [],
            'debug_info': {}
        }

        # –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        if expanded['primary']:
            result['search_query'] = ' '.join(expanded['primary'][:3])

            # –¢–µ—Ä–º–∏–Ω—ã —Å –≤–µ—Å–∞–º–∏
        weight_mapping = {
            'primary': 3.0,  # –°–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –≤–µ—Å
            'secondary': 2.0,  # –í—ã—Å–æ–∫–∏–π –≤–µ—Å
            'tertiary': 1.0  # –°—Ä–µ–¥–Ω–∏–π –≤–µ—Å
        }

        for category, terms in expanded.items():
            base_weight = weight_mapping.get(category, 1.0)
            for i, term in enumerate(terms[:5]):  # –ú–∞–∫—Å–∏–º—É–º 5 —Ç–µ—Ä–º–∏–Ω–æ–≤ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                weight = base_weight - (i * 0.1)  # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–µ–º –≤–µ—Å
                if weight > 0.5:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å
                    result['boost_terms'][term] = round(weight, 2)

        # –í—Å–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        for terms in expanded.values():
            result['all_terms'].extend(terms)
        result['all_terms'] = list(set(result['all_terms']))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏

        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        result['debug_info'] = {
            'tender_name': tender_item.get('name', ''),
            'characteristics_count': len(tender_item.get('characteristics', [])),
            'primary_terms': expanded['primary'][:3],
            'secondary_terms': expanded['secondary'][:5],
            'tertiary_terms': expanded['tertiary'][:3],
            'total_boost_terms': len(result['boost_terms']),
            'total_all_terms': len(result['all_terms'])
        }

        print(f"‚úÖ –§–ò–ù–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
        print(f"   - –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: '{result['search_query']}'")
        print(f"   - –¢–µ—Ä–º–∏–Ω—ã —Å –≤–µ—Å–∞–º–∏: {len(result['boost_terms'])}")
        print(f"   - –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(result['all_terms'])}")

        return result


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
def test_extractor():
    """–¢–µ—Å—Ç —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
    extractor = ConfigurableTermExtractor()

    test_tender = {
        "name": "–ë–ª–æ–∫–∏ –¥–ª—è –∑–∞–ø–∏—Å–µ–π",
        "characteristics": [
            {"name": "–¶–≤–µ—Ç –±—É–º–∞–≥–∏", "value": "–ü–∞—Å—Ç–µ–ª—å–Ω—ã–π"},
            {"name": "–¢–∏–ø", "value": "–° –∫–ª–µ–π–∫–∏–º –∫—Ä–∞–µ–º"},
            {"name": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ –≤ –±–ª–æ–∫–µ", "value": "‚â• 100"}
        ]
    }

    result = extractor.extract_from_tender(test_tender)

    print(f"\nüéØ –¢–ï–°–¢–û–í–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
    print(f"–ó–∞–ø—Ä–æ—Å: {result['search_query']}")
    print(f"Boost terms: {result['boost_terms']}")
    print(f"Debug: {result['debug_info']}")


if __name__ == "__main__":
    test_extractor()